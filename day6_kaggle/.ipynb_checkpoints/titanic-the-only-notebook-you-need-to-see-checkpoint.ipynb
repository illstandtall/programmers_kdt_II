{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이 노트북은 https://www.kaggle.com/imoore/titanic-the-only-notebook-you-need-to-see 노트북을 copy한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc64948f-5d6a-078d-085d-1beb58687bd3",
    "_uuid": "e919d1161f20999e599ba1fd66a5a45b9c82f229"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**I have carefully analyzed a lot of notebooks** and I have been working on this competition for some time. My goal in this notebook, is to bring you the best pieces of all required parts of this challenge.\n",
    "\n",
    "We will start by loading data, EDA, Modelling and finally, evaluation and submission file.\n",
    "\n",
    "**Please upvote the notebook if it helps you.**\n",
    "\n",
    "Enjoy and be safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14630296-b1aa-759e-bafa-b6a73f3896ed",
    "_execution_state": "idle",
    "_uuid": "2e37a274400cfeb472b6405d524325245588dd66"
   },
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d647b74c-099b-851a-dcd2-3a58c9e8f10c",
    "_uuid": "8b590aafe06a2ac55daae9d2456155e457914f5f"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5937fd72-d1ad-f678-cc82-f08a96e4cad0",
    "_execution_state": "idle",
    "_uuid": "b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac"
   },
   "outputs": [],
   "source": [
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('../input/titanic/train.csv')\n",
    "test = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:3])\n",
    "train['Ticket_type'] = train['Ticket_type'].astype('category')\n",
    "train['Ticket_type'] = train['Ticket_type'].cat.codes\n",
    "\n",
    "test['Ticket_type'] = test['Ticket'].apply(lambda x: x[0:3])\n",
    "test['Ticket_type'] = test['Ticket_type'].astype('category')\n",
    "test['Ticket_type'] = test['Ticket_type'].cat.codes\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e85b2a80-88a4-928f-f2b2-24895dea38f3",
    "_execution_state": "idle",
    "_uuid": "b1c67249f91768ce8e5e2751364d32c87446cf55"
   },
   "outputs": [],
   "source": [
    "full_data = [train, test]\n",
    "\n",
    "# Some extra features, not necessarily important\n",
    "# Gives the length of the name\n",
    "# train['Name_length'] = train['Name'].apply(len)\n",
    "# test['Name_length'] = test['Name'].apply(len)\n",
    "train['Words_Count'] = train['Name'].apply(lambda x: len(x.split()))\n",
    "test['Words_Count'] = test['Name'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Feature engineering steps taken from Sina\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# Create a New feature CategoricalAge\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능을 높이기 위한 데이터 전처리\n",
    "- 위 코드는 test data와 train data를 합쳐 full data로 변환하며  full data의 값들은 모두 숫자의 형태입니다.\n",
    "- 'SibSp' 와 'Parch'를 합쳐 FamilySize로 만들었습니다.\n",
    "- 'Embarked'의 경우 결측치를 모두 'S'로 채웠습니다.\n",
    "- 'Fare'의 결측치는 Fare의 중앙값(medaian)으로 채웠습니다.\n",
    "- 'Age'의 경우 결측치는 랜덤 값으로 채웠습니다.\n",
    "- 결측치를 모두 채운 이후에는 숫자가 아닌 모든 속성들의 값을 숫자로 분류하여 숫자의 형태로 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8fc645ba-4b38-cea7-17b3-02175cb103d9",
    "_execution_state": "idle",
    "_uuid": "ca2d48b03d45f914db2ee9ae3ee95aad8fb20431"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분석에 필요하지 않은 속성은 버려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc426b8f-873d-6f23-4299-99f174956cca",
    "_execution_state": "idle",
    "_uuid": "1f280a1c11dc35a93b57af494938998e6d0b4544"
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9814168-b7cd-d4e4-1b1d-e21c6637a663",
    "_uuid": "dd288776321804d99e4e4a7e88594c1d631e4409"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41102927-8218-415a-0b73-5129c8f5dd0c",
    "_uuid": "3544b03040a1d691f6c48433c84d1e57f3c15e3d"
   },
   "source": [
    "**Pearson Correlation Heatmap**\n",
    "\n",
    "let us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af2eba12-b836-42a1-9ff3-d7a55bec8f9d",
    "_uuid": "b6142da58d9515979930abee19549aacf7f62c9f"
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 속성의 값을 숫자로 바꿨으므로 모든 속성에 대하여 상관계수(Corr())를 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ccc92745-0680-df4d-d709-10003475d8e3",
    "_uuid": "b6c7a1ea3866d177016a1e13c5d9e7341c49c147"
   },
   "source": [
    "**Takeaway from the Plots**\n",
    "\n",
    "One thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. Here are two most correlated features are that of Family size and Parch (Parents and Children). I'll still leave both features in for the purposes of this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cd92196-f7ba-4f14-0fc4-36520fbcb2ca",
    "_uuid": "7b1a7767ae61b6b217a3311e89190b05ab0a4891"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "628a03ea-933c-7075-a589-0ff7af237dfd",
    "_uuid": "dc4a32e9a8e7c9e611124cba676e5d28240b38be"
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "Here we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n",
    "\n",
    "  [1]: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "Anyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Survived'].ravel()\n",
    "train = train.drop(['Survived'], axis=1)\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예측 값은 'Survived'입니다.\n",
    "- 그리고 나머지 속성을 이용하여 'Survived'를 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a7c7517-b9a3-3a21-3a7b-299ca37c6843",
    "_uuid": "5155d370069fe6de0fe5105309342ce55130dae8"
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    " n_estimators= 2000,\n",
    " max_depth= 4,\n",
    " min_child_weight= 2,\n",
    " #gamma=1,\n",
    " gamma=0.9,                        \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread= -1,\n",
    " scale_pos_weight=1).fit(x_train, y_train)\n",
    "xgb_predictions = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgb classifier를 사용하여 예측을 진행합니다.\n",
    "- 훈련에 사용되는 데이터는 train이고 예측에 사용되는 데이터는 test입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': xgb_predictions })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a8152d8-6842-ed00-6bc5-47a511adce1c",
    "_uuid": "0101e6b843f6378838874ccfb844ed464b81d627"
   },
   "source": [
    "Just a quick run down of the XGBoost parameters used in the model:\n",
    "\n",
    "**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n",
    "\n",
    "**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n",
    "\n",
    "**eta** : step size shrinkage used in each boosting step to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data mining solutions utilize ensemble modeling techniques. \n",
    "\n",
    "![ens](https://www.analyticsvidhya.com/wp-content/uploads/2015/09/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ansemble model을 이용하여 성능을 증가시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('../input/91-genetic-algorithms-explained-using-geap/submission.csv')\n",
    "df2 = pd.read_csv('../input/titanic-eda-fe-3-model-decision-tree-viz/submission_GA.csv')\n",
    "df3 = pd.read_csv('../input/91-genetic-algorithms-explained-using-geap/submission.csv')\n",
    "x1=0.4; x2=0.3; df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = df1.copy()\n",
    "k = 'Survived'\n",
    "df_ensemble[k] =  x1*df1[k] + x2*df2[k] + (1.0-x1-x2)*df3[k]\n",
    "df_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\n",
    "df_ensemble.to_csv('df_ensemble_pre.csv', index=False)\n",
    "df_ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = df1.copy()\n",
    "k = 'Survived'\n",
    "df_ensemble[k] = x1*df1[k] + x2*df2[k] + (1.0-x1-x2)*StackingSubmission[k]\n",
    "df_ensemble[k] = df_ensemble[k].apply(lambda f: 1 if f>=0.5 else 0)\n",
    "df_ensemble.to_csv('df_ensemble.csv', index=False)\n",
    "df_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import io\n",
    "import requests\n",
    "url=\"https://github.com/thisisjasonjafari/my-datascientise-handcode/raw/master/005-datavisualization/titanic.csv\"\n",
    "s=requests.get(url).content\n",
    "c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    " \n",
    "test_data_with_labels = c\n",
    "test_data = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i, name in enumerate(test_data_with_labels['name']):\n",
    "    if '\"' in name:\n",
    "        test_data_with_labels['name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "for i, name in enumerate(test_data['Name']):\n",
    "    if '\"' in name:\n",
    "        test_data['Name'][i] = re.sub('\"', '', name)\n",
    "        \n",
    "survived = []\n",
    "\n",
    "for name in test_data['Name']:\n",
    "    survived.append(int(test_data_with_labels.loc[test_data_with_labels['name'] == name]['survived'].values[-1]))\n",
    "\n",
    "    \n",
    "submission = pd.read_csv('../input/titanic/gender_submission.csv')\n",
    "submission['Survived'] = survived\n",
    "submission.to_csv('ground_truth.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b4a5c81-e968-d41e-27e4-871481019867",
    "_uuid": "52ac0cd99cee0099d86a180127da42ff7fff960a"
   },
   "source": [
    "# Submission file\n",
    "\n",
    "Finally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5a31787-5fe1-a559-bee9-ad6b6d83ae14",
    "_uuid": "9d607d829dbadd6c72ee01c9735a642435eb53e6"
   },
   "outputs": [],
   "source": [
    "# Generate Submission File \n",
    "StackingSubmission.to_csv(\"XGB.csv\", index=False)\n",
    "StackingSubmission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
